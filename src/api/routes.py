from fastapi import APIRouter, Depends, HTTPException,BackgroundTasks
from src.jobs import scrape_upwork
from sqlalchemy.orm import Session
from src.database import get_db  # è®°å¾—æˆ‘ä»¬ä¹‹å‰å†™çš„ä¾èµ–æ³¨å…¥å—ï¼Ÿç°åœ¨æ´¾ä¸Šç”¨åœºäº†
from src.models import UpworkJob
from typing import List, Optional
from src.api.auth import verify_api_key
router = APIRouter(dependencies=[Depends(verify_api_key)])


# å®šä¹‰è¿”å›çš„æ•°æ®æ ¼å¼ (Pydantic Model) - è¿™æ˜¯ä¸€ä¸ªå¥½çš„å·¥ç¨‹å®è·µï¼Œä½†ä¸ºäº†ç®€å•å…ˆè·³è¿‡ï¼Œç›´æ¥è¿”å› dict

@router.get("/jobs", tags=["Jobs"])
def get_jobs(
        limit: int = 10,
        keyword: Optional[str] = None,
        db: Session = Depends(get_db)
):
    """
    è·å–èŒä½åˆ—è¡¨ (æ”¯æŒå…³é”®è¯è¿‡æ»¤)
    """
    query = db.query(UpworkJob)

    if keyword:
        # æ¨¡ç³Šæœç´¢
        query = query.filter(UpworkJob.search_keyword.ilike(f"%{keyword}%"))

    # æŒ‰æ—¶é—´å€’åº
    jobs = query.order_by(UpworkJob.created_at.desc()).limit(limit).all()

    return jobs  # FastAPI ä¼šè‡ªåŠ¨æŠŠ ORM å¯¹è±¡è½¬æˆ JSON


@router.get("/stats", tags=["Analytics"])
def get_stats(db: Session = Depends(get_db)):
    """
    è·å–ç»Ÿè®¡æ•°æ® (API ç‰ˆ Dashboard)
    """
    total = db.query(UpworkJob).count()
    # ç®€å•çš„ç»Ÿè®¡ç¤ºä¾‹
    return {
        "total_jobs": total,
        "status": "healthy"
    }


# å®šä¹‰ä¸€ä¸ªåå°ä»»åŠ¡å‡½æ•°
def run_crawler_task(keyword: str):
    print(f"ğŸš€ [Background] å¼€å§‹æŠ“å–: {keyword}")
    # è¿™é‡Œè°ƒç”¨ä½ ä¹‹å‰çš„çˆ¬è™«é€»è¾‘ï¼Œç¨å¾®æ”¹é€ ä¸€ä¸‹ scrape_upwork è®©å®ƒæ”¯æŒä¼ å‚
    # scrape_upwork.run_single_keyword(keyword)
    pass


@router.post("/crawl", tags=["Actions"])
def trigger_crawl(
        keyword: str,
        background_tasks: BackgroundTasks
):
    """
    è§¦å‘çˆ¬è™«ä»»åŠ¡ (å¼‚æ­¥æ‰§è¡Œï¼Œä¸ä¼šå¡ä½æ¥å£)
    """
    # æŠŠä»»åŠ¡æ‰”åˆ°åå°å»è·‘ï¼Œç«‹åˆ»ç»™ç”¨æˆ·è¿”å›ç»“æœ
    background_tasks.add_task(run_crawler_task, keyword)

    return {
        "message": f"çˆ¬è™«ä»»åŠ¡å·²å¯åŠ¨: {keyword}",
        "status": "processing"
    }