from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from sqlalchemy.orm import Session
from src.database import get_db
from src.models import UpworkJob
from typing import List, Optional
from src.api.auth import verify_api_key

# å…¨å±€é‰´æƒï¼šè¿™ä¸ªæ–‡ä»¶é‡Œçš„æ‰€æœ‰æ¥å£ï¼Œéƒ½å¿…é¡»å¸¦ API Key
router = APIRouter(dependencies=[Depends(verify_api_key)])


# --- 1. è·å–èŒä½åˆ—è¡¨ ---
@router.get("/jobs", tags=["Jobs"])
def get_jobs(
        limit: int = 10,
        keyword: Optional[str] = None,
        db: Session = Depends(get_db)
):
    """
    è·å–èŒä½åˆ—è¡¨ (æ”¯æŒå…³é”®è¯è¿‡æ»¤)
    """
    query = db.query(UpworkJob)

    if keyword:
        query = query.filter(UpworkJob.search_keyword.ilike(f"%{keyword}%"))

    # æŒ‰æ—¶é—´å€’åº
    jobs = query.order_by(UpworkJob.created_at.desc()).limit(limit).all()
    return jobs


# --- 2. è·å–ç»Ÿè®¡æ•°æ® ---
@router.get("/stats", tags=["Analytics"])
def get_stats(db: Session = Depends(get_db)):
    """
    è·å–æ•°æ®åº“ç»Ÿè®¡æ¦‚è§ˆ
    """
    try:
        total = db.query(UpworkJob).count()
        return {
            "total_jobs": total,
            "status": "healthy",
            "db_connection": "ok"
        }
    except Exception as e:
        return {
            "status": "error",
            "detail": str(e)
        }


# --- 3. è§¦å‘çˆ¬è™« (åå°ä»»åŠ¡) ---

def run_crawler_task(keyword: str):
    """
    å®é™…æ‰§è¡Œçˆ¬è™«çš„é€»è¾‘ (è¿è¡Œåœ¨åå°çº¿ç¨‹)
    """
    print(f"ğŸš€ [Background] æ”¶åˆ°æŠ“å–è¯·æ±‚: {keyword}")

    try:
        # å»¶è¿Ÿå¯¼å…¥ï¼Œé˜²æ­¢ Docker å¯åŠ¨æ—¶å› ç¼º Chrome è€Œå´©æºƒ
        from src.jobs import scrape_upwork

        # è¿™é‡Œè°ƒç”¨çˆ¬è™«çš„ä¸»å…¥å£
        # æ³¨æ„ï¼šç›®å‰çš„ scrape_upwork.run() æ˜¯è·‘å…¨é‡å…³é”®è¯çš„
        # å¦‚æœä½ æƒ³åªè·‘è¿™ä¸€ä¸ª keywordï¼Œä½ éœ€è¦å»æ”¹é€  scrape_upwork.py
        # æš‚æ—¶å…ˆè·‘é»˜è®¤çš„å…¨é‡é€»è¾‘
        scrape_upwork.run()

    except ImportError as e:
        print(f"âŒ ä¸¥é‡é”™è¯¯: æ— æ³•åŠ è½½çˆ¬è™«æ¨¡å— (å¯èƒ½æ˜¯æœåŠ¡å™¨ç¼ºå°‘ Chrome ç¯å¢ƒ): {e}")
    except Exception as e:
        print(f"âŒ çˆ¬è™«è¿è¡Œå‡ºé”™: {e}")


@router.post("/crawl", tags=["Actions"])
def trigger_crawl(
        keyword: str,
        background_tasks: BackgroundTasks
):
    """
    è§¦å‘çˆ¬è™«ä»»åŠ¡ (å¼‚æ­¥)
    """
    # å°†ä»»åŠ¡åŠ å…¥åå°é˜Ÿåˆ—ï¼Œç«‹å³è¿”å›å“åº”
    background_tasks.add_task(run_crawler_task, keyword)

    return {
        "message": f"çˆ¬è™«ä»»åŠ¡å·²æäº¤è‡³åå°é˜Ÿåˆ— (å…³é”®è¯: {keyword})",
        "status": "processing",
        "note": "å¦‚æœæ˜¯äº‘ç«¯ç¯å¢ƒä¸”æœªé…ç½® Chromeï¼Œæ­¤ä»»åŠ¡å¯èƒ½ä¼šå¤±è´¥ï¼Œè¯·æŸ¥çœ‹åå°æ—¥å¿—ã€‚"
    }
